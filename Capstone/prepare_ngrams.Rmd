---
title: "Prediction Model"
author: "Erik Johnson"
date: "1/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadLibraries,message=FALSE,warning=FALSE}
library(data.table)
library(dplyr)
library(quanteda)
library(readtext)
library(lexicon)
library(ggplot2)
```

```{r fileNames}
# Store the data file names for brevity
fileNameBlogs <- "final/en_US/en_US.blogs.txt"
fileNameNews <- "final/en_US/en_US.news.txt"
fileNameTwitter <- "final/en_US/en_US.twitter.txt"

nStart <- 6
nMax <- 6
nGramFileNamesBlogs <- sapply(
    nStart:nMax, 
    function(n) { 
        paste("ngrams/nGramsBlogs", 
              n, 
              "_", 
              letters, 
              ".csv", sep="") 
    })
nGramFileNamesNews <- sapply(
    nStart:nMax, 
    function(n) { 
        paste("ngrams/nGramsNews", 
              n, 
              "_", 
              letters, 
              ".csv", sep="") 
    })
nGramFileNamesTwitter <- sapply(
    nStart:nMax, 
    function(n) { 
        paste("ngrams/nGramsTwitter", 
              n, 
              "_", 
              letters, 
              ".csv", sep="") 
    })
nGramFileNamesAll <- sapply(
    nStart:nMax, 
    function(n) { 
        paste("ngrams/nGramsAll", 
              n, 
              "_", 
              letters, 
              ".csv", sep="") 
    })
fileNameProgress <- "ngrams/prediction_model_progress.txt"
```

```{r getNumLines,cache=TRUE}
numLinesBlogs <- 0
numLinesNews <- 0
numLinesTwitter <- 0
con <- file(fileNameBlogs, "r")
numLinesBlogs <- length(readLines(con))
close(con)
con <- file(fileNameNews, "r")
numLinesNews <- length(readLines(con))
close(con)
con <- file(fileNameTwitter, "r")
numLinesTwitter <- length(readLines(con))
close(con)
```

```{r showNumLines}
numLinesBlogs
numLinesNews
numLinesTwitter
```

```{r readUtilities,echo=FALSE}
knitr::read_chunk("utilities.R")
```
```{r utilityFunctionGetLinesFromFile}
```
```{r utilityFunctionGetDFM}
```
```{r utilityFunctionGetNGramFrequencies}
```
```{r utilityFunctionAppendNGramFrequencies}
```
```{r utilityFunctionLogProgress}
```

```{r initializeFiles}
emptyDF <- data.table(
    ngram = character(),
    base = character(),
    predicted = character(),
    freq = integer())
for (l in letters) {
    sapply(nStart:nMax, function(n) { fwrite(emptyDF, nGramFileNamesBlogs[l == letters, n-nStart+1]) })
    sapply(nStart:nMax, function(n) { fwrite(emptyDF, nGramFileNamesNews[l == letters, n-nStart+1]) })
    sapply(nStart:nMax, function(n) { fwrite(emptyDF, nGramFileNamesTwitter[l == letters, n-nStart+1]) })
    sapply(nStart:nMax, function(n) { fwrite(emptyDF, nGramFileNamesAll[l == letters, n-nStart+1]) })
}
```

```{r processFiles}
gc()
nChunks <- 10
datasetName <- c("blogs", "news", "twitter")
fileNames <- c(fileNameBlogs, fileNameNews, fileNameTwitter)
nGramFileNames <- list(nGramFileNamesBlogs, nGramFileNamesNews, nGramFileNamesTwitter)
chunkSizes <- c(
    ceiling(numLinesBlogs / nChunks), 
    ceiling(numLinesNews / nChunks), 
    ceiling(numLinesTwitter / nChunks))
logProgress("----------", fileNameProgress)
logProgress(paste("nChunks: ", nChunks), fileNameProgress)
logProgress(paste("nMax: ", nMax), fileNameProgress)
for (i in 1:nChunks) {
    logProgress(paste("i: ", i, " of ", nChunks), fileNameProgress)
    gc()
    
    for (id in 1:3) {
        logProgress(paste("--id: ", id), fileNameProgress)
        
        # Dataset
        startLine <- (i-1)*chunkSizes[id] + 1
        endLine <- startLine+chunkSizes[id]-1
        logProgress(paste("-->Start line: ", startLine, "; end line: ", endLine), fileNameProgress)
        f <- file(fileNames[id])
        l <- readLines(f)[startLine:endLine]
        close(f)
        gc()
        #l <- fread(fileNames[id], nrows = chunkSizes[id], skip = startLine-1, header = TRUE, colClasses = c("character"))
        #l <- getLinesFromFile(fileNames[id], startLine, chunkSizes[id])
        c <- corpus(l)
        rm(list=c("l"))
        
        for (n in nStart:nMax) {
            logProgress(paste("----n: ", n, " of ", nMax), fileNameProgress)
            nGramFreqs <- getNGramFrequencies(getDFM(c, n))
            message(str(nGramFreqs))
            
            for (l in letters) {
                logProgress(paste("------l: ", l), fileNameProgress)
                relevantNGrams <- nGramFreqs[ngram %like% paste("^", l, sep="")]
                existingNGramFreqs <- fread(nGramFileNames[[id]][l == letters, n-nStart+1])
                existingNGramFreqs <- appendNGramFrequencies(existingNGramFreqs, relevantNGrams)
                fwrite(existingNGramFreqs, nGramFileNames[[id]][l == letters, n-nStart+1])
                rm(list=c("existingNGramFreqs"))
                gc()
                
                existingNGramAllFreqs <- fread(nGramFileNamesAll[l == letters, n-nStart+1])
                existingNGramAllFreqs <- appendNGramFrequencies(existingNGramAllFreqs, relevantNGrams)
                fwrite(existingN,GramAllFreqs, nGramFileNamesAll[l == letters, n-nStart+1])
                rm(list=c("existing                                        NGramAllFreqs", "relevantNGrams"))
                gc()
                
            }
            
            rm(list=c("nGramFreqs"))
            gc()
            
        }
        rm(list=c("c"))
        gc()
    }
    
}
```


```{r utilityFunctionGetDataSample}
```
```{r utilityFunctionGenerateDataSample}
```

```{r loadRandomLines,cache=TRUE,warning=FALSE}
#p <- 0.1
#set.seed(258179)
#corpusBlogs <- generateDataSample(fileNameBlogs, outputFileNameBlogs, p)
#corpusNews <- generateDataSample(fileNameNews, outputFileNameNews, p)
#corpusTwitter <- generateDataSample(fileNameTwitter, outputFileNameTwitter, p)
#corpusAll <- corpusBlogs + corpusNews + corpusTwitter
```

```{r ngrams, cache=TRUE}
#ngram_counts <- 1:4
#dfmAll1 <- dfm(corpusAll, ngrams=1,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll2 <- dfm(corpusAll, ngrams=2,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll3 <- dfm(corpusAll, ngrams=3,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll4 <- dfm(corpusAll, ngrams=4,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
```

```{r ngramFrequencies,cache=TRUE}
# tokenFreqsAll1 <- data.frame(token = colnames(dfmAll1),
#                                  freq = colSums(dfmAll1),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll2 <- data.frame(token = colnames(dfmAll2),
#                                  freq = colSums(dfmAll2),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll3 <- data.frame(token = colnames(dfmAll3),
#                                  freq = colSums(dfmAll3),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll4 <- data.frame(token = colnames(dfmAll4),
#                                  freq = colSums(dfmAll4),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
```

```{r showMostFrequent}
#topfeatures(dfmAll1, n=50)
#topfeatures(dfmAll2, n=50)
#topfeatures(dfmAll3, n=50)
#topfeatures(dfmAll4, n=50)
```

```{r}
# head(tokenFreqsAll4)
# q1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
# c1 <- corpus(q1)
# dfm1 <- dfm(c1, ngrams=3,
#                 remove_punct = TRUE, 
#                 remove_symbols = TRUE, 
#                 remove_numbers = TRUE, remove=stopwords("english"))
# q1t <- paste("^", tail(colnames(dfm1), n=1), sep="")
# q1t
# q1f <- tokenFreqsAll4[grepl(q1t, tokenFreqsAll4$token),]
# q1f
# head(q1f[order(q1f$freq, decreasing=TRUE),], n=10)
# beer

# q2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
# q2 <- "^mean_the"
# q2f <- tokenFreqsAll4[grepl(q2, tokenFreqsAll4$token),]
# q2f <- tokenFreqsAll3[grepl(q2, tokenFreqsAll3$token),]
# q2f
# world

# 
# q3 <- "Hey sunshine, can you follow me and make me the"
# q3 <- "^me_the"
# q3f <- tokenFreqsAll4[grepl(q3, tokenFreqsAll4$token),]
# q3f <- tokenFreqsAll3[grepl(q3, tokenFreqsAll3$token),]
# q3f
# happiest

# 
# q4 <- "Very early observations on the Bills game: Offense still struggling but the"
# q4 <- "^but_the"
# q4f <- tokenFreqsAll4[grepl(q4, tokenFreqsAll4$token),]
# q4f <- tokenFreqsAll3[grepl(q4, tokenFreqsAll3$token),]
# q4f
# crowd (incorrect)
# defense

# 
# q5 <- "Go on a romantic date at the"
# q5 <- "^at_the"
# q5f <- tokenFreqsAll4[grepl(q5, tokenFreqsAll4$token),]
# q5f <- tokenFreqsAll3[grepl(q5, tokenFreqsAll3$token),]
# q5f
# beach

# 
# q6 <- "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
# q6 <- "^on_my"
# q6f <- tokenFreqsAll4[grepl(q6, tokenFreqsAll4$token),]
# q6f <- tokenFreqsAll3[grepl(q6, tokenFreqsAll3$token),]
# q6f
# way

# 
# q7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
# q7 <- "^quite_some"
# q7f <- tokenFreqsAll4[grepl(q7, tokenFreqsAll4$token),]
# q7f <- tokenFreqsAll3[grepl(q7, tokenFreqsAll3$token),]
# q7f
# time

# 
# q8 <- "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
# q8 <- "^with_his"
# q8f <- tokenFreqsAll4[grepl(q8, tokenFreqsAll4$token),]
# q8f <- tokenFreqsAll3[grepl(q8, tokenFreqsAll3$token),]
# q8f
# finger

# 
# q9 <- "Be grateful for the good times and keep the faith during the"
# q9 <- "^during_the"
# q9f <- tokenFreqsAll4[grepl(q9, tokenFreqsAll4$token),]
# q9f <- tokenFreqsAll3[grepl(q9, tokenFreqsAll3$token),]
# q9f
# hard (incorrect)
# bad

# 
# q10 <- "If this isn't the cutest thing you've ever seen, then you must be"
# q10 <- "^must_be"
# q10f <- tokenFreqsAll4[grepl(q10, tokenFreqsAll4$token),]
# q10f <- tokenFreqsAll3[grepl(q10, tokenFreqsAll3$token),]
# q10f
# insane


# q <- "If this isn't the cutest thing you've ever seen, then you must be"
# c <- corpus(q)
# d <- dfm(c, ngrams=2, remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# qt <- tail(colnames(d), n=1)
# qt
# dt1 <- subset(dt, base == qt)
# dt1[order(-freq)][1:100]

```

