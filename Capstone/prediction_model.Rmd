---
title: "Prediction Model"
author: "Erik Johnson"
date: "1/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadLibraries,message=FALSE,warning=FALSE}
library(quanteda)
library(readtext)
library(lexicon)
library(ggplot2)
```

```{r fileNames}
# Store the data file names for brevity
fileNameBlogs <- "final/en_US/en_US.blogs.txt"
fileNameNews <- "final/en_US/en_US.news.txt"
fileNameTwitter <- "final/en_US/en_US.twitter.txt"

nMax <- 6
nGramFileNamesBlogs <- sapply(1:nMax, function(n) { paste("nGramsBlogs", n, ".csv", sep="") })
nGramFileNamesNews <- sapply(1:nMax, function(n) { paste("nGramsNews", n, ".csv", sep="") })
nGramFileNamesTwitter <- sapply(1:nMax, function(n) { paste("nGramsTwitter", n, ".csv", sep="") })
nGramFileNamesAll <- sapply(1:nMax, function(n) { paste("nGramsAll", n, ".csv", sep="") })
```

```{r getNumLines,cache=TRUE}
numLinesBlogs <- 0
numLinesNews <- 0
numLinesTwitter <- 0
con <- file(fileNameBlogs, "r")
numLinesBlogs <- length(readLines(con))
close(con)
con <- file(fileNameNews, "r")
numLinesNews <- length(readLines(con))
close(con)
con <- file(fileNameTwitter, "r")
numLinesTwitter <- length(readLines(con))
close(con)
```

```{r showNumLines}
numLinesBlogs
numLinesNews
numLinesTwitter
```

```{r readUtilities,echo=FALSE}
knitr::read_chunk("utilities.R")
```
```{r utilityFunctionGetLinesFromFile}
```
```{r utilityFunctionGetDFM}
```
```{r utilityFunctionGetNGramFrequencies}
```
```{r utilityFunctionAppendNGramFrequencies}
```

```{r initializeFiles}
emptyDF <- data.frame(
    ngram = character(),
    base = character(),
    predicted = character(),
    freq = integer(),
    row.names=NULL,
    stringsAsFactors = FALSE)
sapply(1:nMax, function(n) { write.csv(emptyDF, nGramFileNamesBlogs[n]) })
sapply(1:nMax, function(n) { write.csv(emptyDF, nGramFileNamesNews[n]) })
sapply(1:nMax, function(n) { write.csv(emptyDF, nGramFileNamesTwitter[n]) })
sapply(1:nMax, function(n) { write.csv(emptyDF, nGramFileNamesAll[n]) })
```

```{r processFiles,cache=TRUE}
gc()
nChunks <- 10
datasetName <- c("blogs", "news", "twitter")
fileNames <- c(fileNameBlogs, fileNameNews, fileNameTwitter)
nGramFileNames <- list(nGramFileNamesBlogs, nGramFileNamesNews, nGramFileNamesTwitter)
chunkSizes <- c(
    ceiling(numLinesBlogs / nChunks), 
    ceiling(numLinesNews / nChunks), 
    ceiling(numLinesTwitter / nChunks))

for (i in 1:2) {
    gc()
    
    for (id in 1:3) {
        
        # Dataset
        startLine <- (i-1)*chunkSizes[id] + 1
        l <- getLinesFromFile(fileNames[id], startLine, chunkSizes[id])
        c <- corpus(l)
        rm(list=c("l"))
        for (n in 1:nMax) {
            nGramFreqs <- getNGramFrequencies(getDFM(c, n))
            message(str(nGramFreqs))
            
            existingNGramFreqs <- read.csv(nGramFileNames[[id]][n])
            existingNGramFreqs <- appendNGramFrequencies(existingNGramFreqs, nGramFreqs)
            write.csv(existingNGramFreqs, nGramFileNames[[id]][n], row.names = FALSE)
            rm(list=c("existingNGramFreqs"))
            gc()
            
            existingNGramAllFreqs <- read.csv(nGramFileNamesAll[n])
            existingNGramAllFreqs <- appendNGramFrequencies(existingNGramAllFreqs, nGramFreqs)
            write.csv(existingNGramAllFreqs, nGramFileNamesAll[n], row.names = FALSE)
            rm(list=c("existingNGramAllFreqs"))
            gc()
            
        }
        rm(list=c("c"))
        gc()
    }
    
}
```


```{r utilityFunctionGetDataSample}
```
```{r utilityFunctionGenerateDataSample}
```

```{r loadRandomLines,cache=TRUE,warning=FALSE}
#p <- 0.1
#set.seed(258179)
#corpusBlogs <- generateDataSample(fileNameBlogs, outputFileNameBlogs, p)
#corpusNews <- generateDataSample(fileNameNews, outputFileNameNews, p)
#corpusTwitter <- generateDataSample(fileNameTwitter, outputFileNameTwitter, p)
#corpusAll <- corpusBlogs + corpusNews + corpusTwitter
```

```{r ngrams, cache=TRUE}
#ngram_counts <- 1:4
#dfmAll1 <- dfm(corpusAll, ngrams=1,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll2 <- dfm(corpusAll, ngrams=2,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll3 <- dfm(corpusAll, ngrams=3,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
#dfmAll4 <- dfm(corpusAll, ngrams=4,
#                remove_punct = TRUE, 
#                remove_symbols = TRUE, 
#                remove_numbers = TRUE, remove=stopwords("english"))
```

```{r ngramFrequencies,cache=TRUE}
# tokenFreqsAll1 <- data.frame(token = colnames(dfmAll1),
#                                  freq = colSums(dfmAll1),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll2 <- data.frame(token = colnames(dfmAll2),
#                                  freq = colSums(dfmAll2),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll3 <- data.frame(token = colnames(dfmAll3),
#                                  freq = colSums(dfmAll3),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
# tokenFreqsAll4 <- data.frame(token = colnames(dfmAll4),
#                                  freq = colSums(dfmAll4),
#                                  row.names = NULL,
#                                  stringsAsFactors = FALSE)
```

```{r showMostFrequent}
#topfeatures(dfmAll1, n=50)
#topfeatures(dfmAll2, n=50)
#topfeatures(dfmAll3, n=50)
#topfeatures(dfmAll4, n=50)
```

```{r}
# head(tokenFreqsAll4)
# q1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
# c1 <- corpus(q1)
# dfm1 <- dfm(c1, ngrams=3,
#                 remove_punct = TRUE, 
#                 remove_symbols = TRUE, 
#                 remove_numbers = TRUE, remove=stopwords("english"))
# q1t <- paste("^", tail(colnames(dfm1), n=1), sep="")
# q1t
# q1f <- tokenFreqsAll4[grepl(q1t, tokenFreqsAll4$token),]
# q1f
# head(q1f[order(q1f$freq, decreasing=TRUE),], n=10)
# beer

# q2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
# q2 <- "^mean_the"
# q2f <- tokenFreqsAll4[grepl(q2, tokenFreqsAll4$token),]
# q2f <- tokenFreqsAll3[grepl(q2, tokenFreqsAll3$token),]
# q2f
# world

# 
# q3 <- "Hey sunshine, can you follow me and make me the"
# q3 <- "^me_the"
# q3f <- tokenFreqsAll4[grepl(q3, tokenFreqsAll4$token),]
# q3f <- tokenFreqsAll3[grepl(q3, tokenFreqsAll3$token),]
# q3f
# happiest

# 
# q4 <- "Very early observations on the Bills game: Offense still struggling but the"
# q4 <- "^but_the"
# q4f <- tokenFreqsAll4[grepl(q4, tokenFreqsAll4$token),]
# q4f <- tokenFreqsAll3[grepl(q4, tokenFreqsAll3$token),]
# q4f
# crowd (incorrect)
# defense

# 
# q5 <- "Go on a romantic date at the"
# q5 <- "^at_the"
# q5f <- tokenFreqsAll4[grepl(q5, tokenFreqsAll4$token),]
# q5f <- tokenFreqsAll3[grepl(q5, tokenFreqsAll3$token),]
# q5f
# beach

# 
# q6 <- "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
# q6 <- "^on_my"
# q6f <- tokenFreqsAll4[grepl(q6, tokenFreqsAll4$token),]
# q6f <- tokenFreqsAll3[grepl(q6, tokenFreqsAll3$token),]
# q6f
# way

# 
# q7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
# q7 <- "^quite_some"
# q7f <- tokenFreqsAll4[grepl(q7, tokenFreqsAll4$token),]
# q7f <- tokenFreqsAll3[grepl(q7, tokenFreqsAll3$token),]
# q7f
# time

# 
# q8 <- "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
# q8 <- "^with_his"
# q8f <- tokenFreqsAll4[grepl(q8, tokenFreqsAll4$token),]
# q8f <- tokenFreqsAll3[grepl(q8, tokenFreqsAll3$token),]
# q8f
# finger

# 
# q9 <- "Be grateful for the good times and keep the faith during the"
# q9 <- "^during_the"
# q9f <- tokenFreqsAll4[grepl(q9, tokenFreqsAll4$token),]
# q9f <- tokenFreqsAll3[grepl(q9, tokenFreqsAll3$token),]
# q9f
# hard (incorrect)
# bad

# 
# q10 <- "If this isn't the cutest thing you've ever seen, then you must be"
# q10 <- "^must_be"
# q10f <- tokenFreqsAll4[grepl(q10, tokenFreqsAll4$token),]
# q10f <- tokenFreqsAll3[grepl(q10, tokenFreqsAll3$token),]
# q10f
# insane


# q <- "If this isn't the cutest thing you've ever seen, then you must be"
# c <- corpus(q)
# d <- dfm(c, ngrams=2, remove_punct=TRUE, remove_symbols=TRUE, remove_numbers=TRUE)
# qt <- tail(colnames(d), n=1)
# qt
# dt1 <- subset(dt, base == qt)
# dt1[order(-freq)][1:100]

```

